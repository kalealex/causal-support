---
title: "E1: Provisional Model Expansion"
author: "Alex Kale"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
library(ggplot2)
library(tidybayes)
library(modelr)
library(RColorBrewer)
```

## Model expansion: Building out a linear in log odds model

Load data.

```{r}
df <- read_csv("e1-anonymous.csv")

head(df)
```

Calculate a log response ratio `lrr` to model as a function of `causal_support`. Also, convert predictor variables to factors for modeling if need be.

```{r}
model_df <- df %>%
  # drop practice trial
  filter(trial != "practice") %>%
  mutate(
    # response units
    response_A =  if_else(
      response_A > 99.5, 99.5,
      if_else(
        response_A < 0.5, 0.5,
        as.numeric(response_A))),
    response_B =  if_else(
      response_B > 99.5, 99.5,
      if_else(
        response_B < 0.5, 0.5,
        as.numeric(response_B))),
    lrr = log(response_A / 100) - log(response_B / 100),
    # predictors as factors
    worker = as.factor(workerId),
    vis = as.factor(condition),
    n = as.factor(n),
    # derived predictors
    delta_p = (count_nGnT + count_GnT)/(total_nGnT + total_GnT) - (count_nGT + count_GT)/(total_nGT + total_GT),
    interactions = if_else(interactions == "placeholder", list(NA), str_split(interactions, "_")),
    trial = as.numeric(trial),
    trial_n = (trial - mean(trial)) / max(trial) # normalized trial indices
  ) %>%
  rowwise() %>%
  mutate(interact = !any(is.na(unlist(interactions)))) %>% # boolean to code for any interaction whatsoever
  unite("vis_interact", vis, interact, remove = FALSE)
```

Let's exclude workers who miss the trial where causal support is the largest. We define miss as absolute error greater than 50%. This mean conditioning on only one of our attention checks to exclude about 21% of participants, rather than conditioning on both attention checks as preregistered which would exclude 47% of participants. 47% is too much, and this reflectst the fact that we underestimated the difficulty of our second attention check trial, where causal support is at a minimum.

```{r}
exclude_df <- model_df %>%
  group_by(workerId) %>%
  summarise(
    max_trial_idx = which(trial_idx == -1)[1],
    max_trial_gt = ground_truth[[max_trial_idx]],
    max_trial_err = abs_err[[max_trial_idx]],
    # min_trial_idx = which(trial_idx == -2)[1],
    # min_trial_gt = ground_truth[[min_trial_idx]],
    # min_trial_err = abs_err[[min_trial_idx]],
    exclude = max_trial_err > 0.5
  )
  

head(exclude_df)
```

Apply the exclusion criteria.

```{r}
model_df = exclude_df %>%
  select(workerId, exclude) %>%
  full_join(model_df, by = "workerId") %>%
  filter(!exclude) %>%
  select(-exclude)
```

Additionally, we'll drop all attention check trials now that we are done using them for exclusions. There was a bug in the interface that led to duplicate attention check trials for the first 135 participants (those workers whose data we analyze here). Because of the bug, filtering out attention checks means dropping more trials than it should, and meaning we will have more trials per participant after the bug is fixed.

```{r}
model_df = model_df %>%
  filter(trial_idx != -1 & trial_idx != -2)
```

Now we're ready to build some models.


## Simple slope and intercept

Prior predictive check

```{r}
p1 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  sample_prior = "only",
  iter = 2000, warmup = 500, chains = 2, cores = 2)
```

```{r}
model_df %>%
  select(causal_support, response_A) %>%
  add_predicted_draws(p1, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Prior predictive distribution") +
    theme(panel.grid = element_blank())
```
It looks like there are some data points that out model thinks are implausible but not impossible. I'm guessing this is because the model specification itself is too simple to account for the data, and fiddling with the priors wouldn't completely fix it.


Fit model

```{r}
m1 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  file = "model-fits/1_simple")
```

Diagnostics

```{r}
summary(m1)
```

```{r}
plot(m1)
```

```{r}
pairs(m1)
```

Posterior predictive

```{r}
model_df %>%
  select(causal_support, response_A) %>%
  add_predicted_draws(m1, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank())
```

The model fit looks good by the diagnostics above, but this model is just a simple linear model on casual support. As we can see in the posterior predictive distribution, it doesn't account for the complexity of our data generating process (as it would if people's behavior was completely normative).


## Adding effects of data manipulations

Now, we'll add predictors for our data manipulations operationalized as delta p and sample size. These are factors whose effects on judgments should be fully mediated by normative causal support in an ideal observer, but the probably have some direct impact on judgments as well in the form of perceptual biases, such as the tendency to underestimate sample size.

Prior predictive check

```{r}
p2 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support*delta_p*n,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  sample_prior = "only",
  iter = 2000, warmup = 500, chains = 2, cores = 2)
```

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 50))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n)) %>%
  add_predicted_draws(p2, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Prior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ .)
```

These prior predictive distributions are already assigning more density to the empirical distribution of the data, which is a good sign.

Fit model

```{r}
m2 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support*delta_p*n,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  file = "model-fits/2_data-conds")
```

Diagnostics

```{r}
summary(m2)
```

```{r}
plot(m2)
```

```{r}
# intercepts
pairs(m2, pars = c("b_Intercept", "b_delta_p", "b_n500", "b_n1000", "b_n1500"))
```

```{r}
# slopes on causal support
pairs(m2, pars = c("b_causal_support", "b_causal_support:delta_p", "b_causal_support:n500", "b_causal_support:n1000", "b_causal_support:n1500"))
```

```{r}
# slopes on delta_p and sigma
pairs(m2, pars = c("b_delta_p", "b_delta_p:n500", "b_delta_p:n1000", "b_delta_p:n1500", "sigma"))
```

```{r}
# slope interactions
pairs(m2, pars = c("b_causal_support:delta_p", "b_causal_support:delta_p:n500", "b_causal_support:delta_p:n1000", "b_causal_support:delta_p:n1500"))
```

As expected, we see some correlation among our slope effects, but it's not enough to raise alarm. It doesn't look like causal support is multicollinear with delta_p or n as it would be if these predictors were redundant.

Posterior predictive

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 50))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n)) %>%
  add_predicted_draws(m2, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ .)
```

This model looks much better than the last one, but there are still data points that get assigned low density in the posterior predictive. This suggests we can still improve our model specification by adding other predictors to our model.


## Adding effects of visualization conditions

Now, we'll let our model know about different visualization conditions that the participants saw.

Prior predictive check

```{r}
p3 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support*delta_p*n*vis,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  sample_prior = "only",
  iter = 2000, warmup = 500, chains = 2, cores = 2)
```

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 50))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n),
    vis = unique(model_df$vis)) %>%
  add_predicted_draws(p3, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Prior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ vis)
```

Fit model

```{r}
m3 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support*delta_p*n*vis,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  file = "model-fits/3_vis")
```

Diagnostics

```{r}
summary(m3)
```

```{r}
plot(m3)
```

Too many pairs to plot, so we'll only do it as needed to check issues from here on out.

Posterior predictive

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n),
    vis = unique(model_df$vis)) %>%
  add_predicted_draws(m3, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ vis)
```

Addition visualization conditions seems to help account for the empirical distribution of the data, but we can probably do even better by adding some parameters to account for things like unequal variances across levels of the ground truth, learning across trials, and individual differences.


## Adding effects of interaction with the visaulizations

Before we get too fancy, let's add the last factor that is core to answering our research questions: whether or not users interacted with the visualization conditions. In interactive conditions, we expect to see some change in performance on trials where users actually took the time to interact with the visualization as intended.

We use the same priors as before, so we can skip the prior predictive check.

Fit model

```{r}
m4 <- brm(data = model_df, family = "gaussian",
  lrr ~ causal_support*delta_p*n*vis_interact,
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = sigma)),                   # weakly informative half-normal
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  file = "model-fits/4_interact")
```

Diagnostics

```{r}
summary(m4)
```

```{r}
plot(m4)
```

Posterior predictive

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 50))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n),
    vis_interact = unique(model_df$vis_interact)) %>%
  add_predicted_draws(m4, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ vis_interact)
```

As expected, differentiating between vis conditions based on whether or not users interacted seems to make a difference. Note that this only changes the model for the aggbars and filtbars conditions.


## Adding variance effects

In the plots above, we can see that variance increases as causal support moves away from 0 (i.e., ground truth response moves away from 0.5). Let's tell our model that this happens by adding a submodel on sigma.

Prior predictive check

```{r}
p5 <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact,
               sigma ~ abs(causal_support)),
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = b, dpar = sigma),          # weakly informative half-normal
            prior(normal(0, 1), class = Intercept, dpar = sigma)), # weakly informative half-normal
  sample_prior = "only",
  iter = 2000, warmup = 500, chains = 2, cores = 2)
```

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 50))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n),
    vis_interact = unique(model_df$vis_interact)) %>%
  add_predicted_draws(p5, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Prior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ vis_interact)
```

These look very weakly informative which is good. Don't read into the line representing the modal density. These prior predictive distributions are not fit to the data.

Fit model

```{r}
m5 <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact,
               sigma ~ abs(causal_support)),
  prior = c(prior(normal(-0.0065, 1), class = Intercept),         # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = b, dpar = sigma),          # weakly informative half-normal
            prior(normal(0, 1), class = Intercept, dpar = sigma)), # weakly informative half-normal
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  file = "model-fits/5a_var")
```

Diagnostics

```{r}
summary(m5)
```

```{r}
plot(m5)
```

Posterior predictive

```{r}
expand_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 50))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    n = unique(model_df$n),
    vis_interact = unique(model_df$vis_interact)) %>%
  add_predicted_draws(m5, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_grid(n ~ vis_interact)
```

This is looking slightly better, but the difference is subtle.


## Adding learning effects

Let's add learning effects to our model. These allow the slope on causal support and the intercept (i.e., avg response) to vary as a function of the trial number. 
<!-- We let these effects interact with visualization conditions to allow for different learning rates with different visual representations.  -->
<!-- We also allow the slope and intercept in the sigma submodel to vary as a function of trial. -->

We use the same priors as before.

Fit model

```{r eval=FALSE}
m6 <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact + causal_support*trial_n, #+ causal_support*vis_interact*trial_n,
               sigma ~ abs(causal_support)), #+ trial_n), #abs(causal_support)*trial_n),
  prior = c(prior(normal(-0.0065, 1), class = Intercept),          # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 1), class = b),                        # center predictor effects at 0
            prior(normal(1, 2), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 1), class = b, dpar = sigma),          # weakly informative half-normal
            prior(normal(0, 1), class = Intercept, dpar = sigma)), # weakly informative half-normal
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/6_learning")
```

<!-- Diagnostics -->

<!-- ```{r} -->
<!-- summary(m6) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- plot(m6) -->
<!-- ``` -->

<!-- Posterior predictive -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(n, vis_interact, trial) %>% -->
<!--   data_grid( -->
<!--     causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))), -->
<!--     delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 10)))) %>% -->
<!--   add_predicted_draws(m6, prediction = "lrr_rep", seed = 1234, n = 500) %>% -->
<!--   mutate( -->
<!--     # transform to probability units -->
<!--     response_A_rep = plogis(lrr_rep) * 100 -->
<!--   ) %>% -->
<!--   ggplot(aes(x = causal_support, y = response_A_rep, fill = vis)) + -->
<!--     stat_lineribbon(.width = c(0.95, 0.8, 0.5)) + -->
<!--     geom_point(data = model_df, aes(y = response_A), alpha = 0.5) + -->
<!--     scale_fill_brewer(type = "qual", palette = 2) + -->
<!--     scale_color_brewer(type = "qual", palette = 2) + -->
<!--     labs(subtitle = "Posterior predictive distribution") + -->
<!--     theme(panel.grid = element_blank()) + -->
<!--     facet_wrap(n ~ vis_interact) -->
<!-- ``` -->

I tried multiple model specifications incorporating learning effects, and none of them would fit. For each model, at least one of the chains seemed to get stuck, which usually indicates a need to re-parameterize. This might mean that effects of learning are small or not present in our data. We will drop learning effects from our path of model expansion.


## Adding random effects on slope and intercept

Now, we'll add some relatively simple random effects per worker to our model, making it hierarchical. This will enable the model to account for individual differences and not confuse them with signal.

Prior predictive check. Now that we've added hierarchy we'll narrow our priors to get a bit of regularization.

```{r}
p7 <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact + (causal_support|workerId),
               sigma ~ abs(causal_support) + (1|workerId)),
  prior = c(prior(normal(-0.0065, 1), class = Intercept),            # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 0.5), class = b),                        # center predictor effects at 0
            prior(normal(1, 0.5), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 0.2), class = b, dpar = sigma),          # weakly informative half-normal
            prior(normal(0, 0.2), class = Intercept, dpar = sigma),  # weakly informative half-normal
            prior(normal(0, 0.2), class = sd),                       # weakly informative half-normal
            prior(lkj(4), class = cor)),                             # avoiding large correlations
  sample_prior = "only",
  iter = 2000, warmup = 500, chains = 2, cores = 2)
```

```{r}
model_df %>% 
  group_by(n, vis_interact, workerId) %>%
  data_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 10))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 10)))) %>%
  add_predicted_draws(p7, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Prior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_wrap(n ~ vis_interact)
```

Fit model.

```{r}
m7 <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact + (causal_support|workerId),
               sigma ~ abs(causal_support) + (1|workerId)),
  prior = c(prior(normal(-0.0065, 1), class = Intercept),            # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 0.5), class = b),                        # center predictor effects at 0
            prior(normal(1, 0.5), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 0.2), class = b, dpar = sigma),          # weakly informative half-normal
            prior(normal(0, 0.2), class = Intercept, dpar = sigma),  # weakly informative half-normal
            prior(normal(0, 0.2), class = sd),                       # weakly informative half-normal
            prior(lkj(4), class = cor)),                             # avoiding large correlations
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/7_re-simple")
```

Diagnostics

```{r}
summary(m7)
```

```{r}
plot(m7)
```

Posterior predictive

```{r}
model_df %>% 
  group_by(n, vis_interact, workerId) %>%
  data_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 10)))) %>%
  add_predicted_draws(m7, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_wrap(n ~ vis_interact)
```



## Adding random effects for interactions of within-subjects manipulations

Now, we make our random effects more complex by allowing the impacts of our within-subjects data manipulations (i.e., delta p and sample size) to vary across individuals. This tells our model that not every worker has exactly the same perceptual biases.

Same priors as before.

Fit model. 

```{r}
m8 <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact + (causal_support*delta_p*n|workerId),
               sigma ~ abs(causal_support) + (1|workerId)),
  prior = c(prior(normal(-0.0065, 1), class = Intercept),            # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 0.5), class = b),                        # center predictor effects at 0
            prior(normal(1, 0.5), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 0.2), class = b, dpar = sigma),          # weakly informative half-normal
            prior(normal(0, 0.2), class = Intercept, dpar = sigma),  # weakly informative half-normal
            prior(normal(0, 0.2), class = sd),                       # weakly informative half-normal
            prior(lkj(4), class = cor)),                             # avoiding large correlations
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/8_re-within")
```

Diagnostics

```{r}
summary(m8)
```

```{r}
plot(m8)
```

Posterior predictive

```{r}
model_df %>% 
  group_by(n, vis_interact, workerId) %>%
  data_grid(
    causal_support = quantile(model_df$causal_support, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20))),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 10)))) %>%
  add_predicted_draws(m8, prediction = "lrr_rep", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    response_A_rep = plogis(lrr_rep) * 100
  ) %>%
  ggplot(aes(x = causal_support, y = response_A_rep)) +
    stat_lineribbon(.width = c(0.95, 0.8, 0.5)) +
    geom_point(data = model_df, aes(y = response_A), alpha = 0.5) +
    scale_fill_brewer() +
    labs(subtitle = "Posterior predictive distribution") +
    theme(panel.grid = element_blank()) +
    facet_wrap(n ~ vis_interact)
```




## Model comparison

```{r}
loo(m1, m2, m3, m4, m5, m7, m8)
```



## Effects of interest as linear in log odds slopes

Derive linear in log odds slopes

```{r}
slopes_df <- model_df %>%
  group_by(n, vis_interact, workerId) %>%
  data_grid(
    causal_support = c(0, 1),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20)))) %>%
  add_fitted_draws(m8, value = "lrr_rep", seed = 1234, n = 500, re_formula = NA) %>%
  compare_levels(lrr_rep, by = causal_support) %>%
  rename(slope = lrr_rep)
```

Effect of sample size

```{r}
slopes_df %>%
  group_by(n, .draw) %>%                     # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize out delta_p and vis effects
  ggplot(aes(x = n, y = slope)) +
    stat_halfeye() +
    theme_bw()
```

Interaction of visualization and sample size

```{r}
slopes_df %>%
  group_by(n, vis_interact, .draw) %>%                 # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize out delta_p effects
  ggplot(aes(x = n, y = slope)) +
    stat_halfeye() +
    theme_bw() +
    facet_grid(. ~ vis_interact)
```

Pattern of slopes across levels of delta_p

```{r}
slopes_df %>%
  group_by(delta_p, .draw) %>%                # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize out n and vis effects
  ggplot(aes(x = delta_p, y = slope, group = .draw)) +
    geom_line(alpha = 0.1) +
    theme_bw()
```

Interaction of visualization and delta_p

```{r}
slopes_df %>%
  group_by(delta_p, vis_interact, .draw) %>%                # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize out n and vis effects
  ggplot(aes(x = delta_p, y = slope, group = .draw)) +
    geom_line(alpha = 0.1) +
    theme_bw() + 
    facet_grid(. ~ vis_interact)
```

Effect of visualization

```{r}
slopes_df %>%
  group_by(vis_interact, .draw) %>%                 # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize out delta_p effects
  ggplot(aes(x = vis_interact, y = slope)) +
    stat_halfeye() +
    theme_bw()
```

Let's look at contrast to see reliability of these visualization effects. We'll flip the cooridinates so we have more space to put the labels for each pairwise difference.

```{r}
slopes_df %>%
  group_by(vis_interact, .draw) %>%                 # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize out delta_p effects
  compare_levels(slope, by = vis_interact) %>%
  ggplot(aes(x = slope, y = vis_interact)) +
    stat_halfeyeh() +
    theme_bw() +
    labs(
      x = "Slope diff",
      y = "Contrast"
    )
```
