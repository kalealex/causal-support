---
title: "E1 Results"
author: "Alex Kale"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
library(ggplot2)
library(tidybayes)
library(modelr)
library(RColorBrewer)
```

## Results

In this document, we present the results of our first experiement. This is intended as a supplement to the full paper. Here, we gather information from our exploratory analysis and model expansion processes, with an emphasis on what we've learned and an eye toward what should be presented in the paper.


### Analysis overview

Our primary research questions focus on how visualization conditions impact the correspondence between user responses and our normative benchmark *causal support*. Causal support says how much a chart user should believe in alternative causal explanations given a data set. In our first experiment, we ask users to differentiate between two alternative causal models, one with a treatment effect (explanation A) and one without a treatment effect (explanation B). 

We estimate the correspondence between user responses and our normative benchmark using a linear in log odds (LLO) model, where ideal performance is a one-to-one relationship between a user's responses and normative causal support. We chartacterize performance primarily in terms of LLO slopes with respect to causal support, which is a measure of sensitivity to the signal in each data set that should support causal inferences. The LLO also has an intercept term, which measures the average response when there is no signal to support either causal explanation. Intercepts represent an overall bias in responses to the extent that they deviate from 50%.

We look at how LLO slopes and intercepts very as a function of visualization condition. In interactive visualization conditions, we separate trials depending on whether or not users interacted with the visualization, which reduces statistical power in these conditions but enables us to be more accurate in estimating the effects of interactive visualizations on causal inferences. At the end of the document we follow up and do a descriptive analysis of how chart users interacted with these visualizations, specifically, which views of the data they chose to create.


### Prepare data

Load data.

```{r}
df <- read_csv("e1-anonymous.csv")

head(df)
```

Calculate a log response ratio `lrr` to model as a function of `causal_support`. Also, convert predictor variables to factors for modeling if need be.

```{r}
model_df <- df %>%
  # drop practice trial
  filter(trial != "practice") %>%
  mutate(
    # response units
    response_A =  if_else(
      response_A > 99.5, 99.5,
      if_else(
        response_A < 0.5, 0.5,
        as.numeric(response_A))),
    response_B =  if_else(
      response_B > 99.5, 99.5,
      if_else(
        response_B < 0.5, 0.5,
        as.numeric(response_B))),
    lrr = log(response_A / 100) - log(response_B / 100),
    # predictors as factors
    worker = as.factor(workerId),
    vis = as.factor(condition),
    n = as.factor(n),
    # derived predictors
    delta_p = (count_nGnT + count_GnT)/(total_nGnT + total_GnT) - (count_nGT + count_GT)/(total_nGT + total_GT),
    interactions_processed = if_else(interactions == "placeholder", list(NA), str_split(interactions, "_")),
    trial = as.numeric(trial),
    trial_n = (trial - mean(trial)) / max(trial) # normalized trial indices
  ) %>%
  rowwise() %>%
  mutate(interact = !any(is.na(unlist(interactions_processed)))) %>% # boolean to code for any interaction whatsoever
  unite("vis_interact", vis, interact, remove = FALSE)
```

Let's exclude workers who miss the trial where causal support is the largest. We define miss as absolute error greater than 50%. This mean conditioning on only one of our attention checks to exclude about 22% of participants, rather than conditioning on both attention checks as preregistered which would exclude 48% of participants. 48% is too much, and this reflects the fact that we underestimated the difficulty of our second attention check trial, where causal support is at a minimum. Here, we are departing from our preregistration, but we are doing so in a way that admits more noise into our sample and is thus a more conservative analysis decision than the exclusion criteria we preregistered.

```{r}
exclude_df <- model_df %>%
  group_by(workerId) %>%
  summarise(
    max_trial_idx = which(trial_idx == -1)[1],
    max_trial_gt = ground_truth[[max_trial_idx]],
    max_trial_err = abs_err[[max_trial_idx]],
    exclude = max_trial_err > 0.5
  )

head(exclude_df)
```

Apply the exclusion criteria.

```{r}
model_df = exclude_df %>%
  select(workerId, exclude) %>%
  full_join(model_df, by = "workerId") %>%
  filter(!exclude) %>%
  select(-exclude)
```

Additionally, we'll drop all attention check trials now that we are done using them for exclusions. Because of a bug that inserted extra attention check trials for the first 135 workers, this means dropping more trials than it should for this subset of workers. Thus, we have more trials per participant after the bug was fixed.

```{r}
model_df = model_df %>%
  filter(trial_idx != -1 & trial_idx != -2)
```

How many participants per condition after exclusions? (target sample size was 80 per condition)

```{r}
model_df %>%
  group_by(vis) %>%
  summarise(
    n = length(unique(workerId))
  )
```

We overshot our target sample size slightly in all but one condition. This happened because we launch HITs in batches on MTurk, and it is hard to anticipate how many people in a batch will pass the exclusion criterion. The few extra participants should not make much of a difference in our results.


### Inferential model

This is the model that we will use statistical inferences, and it is the result of our preregistered model expansion process. See ModelExpansion.Rmd for more information about how we arrived at this model.

```{r}
m <- brm(data = model_df, family = "gaussian",
  formula = bf(lrr ~ causal_support*delta_p*n*vis_interact + (causal_support*delta_p + causal_support*n|workerId)),
  prior = c(prior(normal(-0.1654036, 1), class = Intercept),         # center at mean(qlogis(model_df$response_A / 100))
            prior(normal(0, 0.5), class = b),                        # center predictor effects at 0
            prior(normal(1, 0.5), class = b, coef = causal_support), # center at unbiased slope
            prior(normal(0, 0.5), class = sigma),                    # weakly informative half-normal
            prior(normal(0, 0.5), class = sd),                       # weakly informative half-normal
            prior(lkj(4), class = cor)),                             # avoiding large correlations
  iter = 3000, warmup = 500, chains = 2, cores = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/8_re-within")
```


```{r}
summary(m)
```

### Main effects of visualization

*Recall:* We estimate the correspondence between user responses and our normative benchmark using a linear in log odds (LLO) model, where ideal performance is a one-to-one relationship between a user's responses and normative causal support. We chartacterize performance primarily in terms of LLO slopes with respect to causal support, which is a measure of sensitivity to the signal in each data set that should support causal inferences. The LLO also has an intercept term, which measures the average response when there is no signal to support either causal explanation. Intercepts represent an overall bias in responses to the extent that they deviate from 50%.

To start, lets derive slopes and intercepts from our model.

```{r}
# extract conditional expectations from model
results_df <- model_df %>%
  group_by(n, vis_interact, workerId) %>%
  data_grid(
    causal_support = c(0, 1),
    delta_p = quantile(model_df$delta_p, probs = plogis(seq(from = qlogis(0.001), to = qlogis(0.999), length.out = 20)))) %>%
  add_fitted_draws(m, value = "lrr_rep", seed = 1234, n = 500, re_formula = NA) %>%
  select(-one_of(c(".row",".chain",".iteration")))

# derive slopes
slopes_df <- results_df %>%
  compare_levels(lrr_rep, by = causal_support) %>%
  rename(slope = lrr_rep)

# derive intercepts and merge dataframes
results_df <- results_df %>%
  filter(causal_support == 0) %>%
  rename(intercept = lrr_rep) %>%
  full_join(slopes_df, by = c("n", "vis_interact", "workerId", "delta_p", ".draw"))
```

Let's also set the level order for our visualization conditions for plotting.

```{r}
# relevel vis conditions to control plotting order
results_df <- results_df %>%
  mutate(
    vis_order = case_when(
      as.character(vis_interact) == "text_FALSE"     ~ 1,
      as.character(vis_interact) == "icons_FALSE"    ~ 2,
      as.character(vis_interact) == "bars_FALSE"     ~ 3,
      as.character(vis_interact) == "aggbars_FALSE"  ~ 4,
      as.character(vis_interact) == "aggbars_TRUE"   ~ 5,
      as.character(vis_interact) == "filtbars_FALSE" ~ 6,
      as.character(vis_interact) == "filtbars_TRUE"  ~ 7,
      TRUE                                           ~ 0
    ),
    vis_interact = reorder(vis_interact, vis_order)
  ) 
```

Let's look at posterior estimates of the slope and intercept in each visualization condition.

We'll start with *slopes*.

```{r}
results_df %>%
  group_by(vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%  # marginalize
  ggplot(aes(x = reorder(vis_interact, vis_order), y = slope)) +
    stat_halfeye() +
    theme_bw()
```
```{r eval=FALSE}
# figure for paper
plt <- results_df %>%
  group_by(vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%  # marginalize
  ggplot(aes(x = slope, y = reorder(vis_interact, desc(vis_order)))) +
    stat_slabh() +
    theme_minimal() +
    theme(
      axis.title = element_blank(),
      panel.grid.minor = element_blank()
    )

plt
ggsave("../../figures/components/e1-slopes.svg", plt, width = 3, height = 2)
```
 
Let's look at pairwise contrasts to see reliability of these visualization effects on LLO slopes. We'll flip the cooridinates so we have more space to put the labels for each pairwise difference.

```{r}
slopes_df %>%
  group_by(vis_interact, .draw) %>%           # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>% # marginalize
  compare_levels(slope, by = vis_interact) %>%
  ggplot(aes(x = slope, y = vis_interact)) +
    stat_halfeyeh() +
    theme_bw() +
    labs(
      x = "Slope diff",
      y = "Contrast"
    )
```
 
We can see that icons, bars, and text outperform the other visualization conditions with LLO slopes closer to 1. Differences between these conditions are not reliable. Overall, slopes are far from 1 in all conditions reflecting the difficulty of causal inference as a task.

Interestingly, bars without interaction reliably outperform aggbars and filtbars where people did interact. Users actually perform worse with aggbars when they do take the time to interact with the visualization which is unexpected, although this difference is not reliable. Users do pretty terrible with filtbars, but their performance improves when they take the time to interact, although this difference is also not reliable. Overall, chart users did reliably better with the table format visualizations than with the filtbars, which is expected considering that the information they need for the task is hidden behind clicks with filtbars.

Now, let's look at *intercepts*.
 
```{r}
results_df %>%
  group_by(vis_interact, vis_order, .draw) %>%        # group by predictors to keep
  summarise(intercept = weighted.mean(intercept)) %>% # marginalize 
  ggplot(aes(x = reorder(vis_interact, vis_order), y = intercept)) +
    stat_halfeye() +
    geom_hline(yintercept = qlogis(0.5), color = "red") +
    theme_bw()
```

As before, let's look at pairwise contrasts to see reliability of these visualization effects on LLO intercepts.

```{r}
results_df %>%
  group_by(vis_interact, .draw) %>%                   # group by predictors to keep
  summarise(intercept = weighted.mean(intercept)) %>% # marginalize 
  compare_levels(intercept, by = vis_interact) %>%
  ggplot(aes(x = intercept, y = vis_interact)) +
    stat_halfeyeh() +
    theme_bw() +
    labs(
      x = "Intercept diff",
      y = "Contrast"
    )
```

These intercepts show substantial response bias when there is no signal to support either causal explanation. People consistently overestimate the treatment effect with filtbars and underestimate it in all conditions using the table layout.

Icons lead to reliably more bias than text, bars, and aggbars. Text, bars, and aggbars are not reliably different from each other in term of bias.

Effects of interaction are not reliable, but overestimation bias seems to increase when users interact with filtbars.

*We can also frame these slopes and intercepts in terms of the response scale.*

On the response scale, *slopes* are a change in the average user's subjective probability that there is a treatment effect given an increase in ground truth from `plogis(0) = 0.50` to `plogis(1) = 0.73`. A slope of 1 corresponds to an increase of 23% in the normative probability of a treatment effect.

```{r}
results_df %>%
  group_by(vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(                                   # marginalize
    slope = weighted.mean(slope),
    intercept = weighted.mean(intercept)
  ) %>% 
  mutate(
    response_A_rep_diff = (plogis(slope + intercept) - plogis(intercept)) * 100
  ) %>%
  ggplot(aes(x = reorder(vis_interact, vis_order), y = response_A_rep_diff)) +
    stat_halfeye() +
    geom_hline(yintercept = 23, color = "red") +
    theme_bw()
```

This view of the data reiterates that people are far less sensitive to the signal in the charts than they should be.

On the response scale, *intercepts* are just the average response where the ground truth is `plogis(0) = 0.5`. A response of 50% is ideal when there is no signal in the data.

```{r}
results_df %>%
  group_by(vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(                                   # marginalize
    intercept = weighted.mean(intercept)
  ) %>% 
  mutate(
    response_A_rep = plogis(intercept) * 100
  ) %>%
  ggplot(aes(x = reorder(vis_interact, vis_order), y = response_A_rep)) +
    stat_halfeye() +
    geom_hline(yintercept = 50, color = "red") +
    theme_bw()
```

```{r eval=FALSE}
# figure for paper
plt <- results_df %>%
  group_by(vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(                                   # marginalize
    intercept = weighted.mean(intercept)
  ) %>% 
  mutate(
    response_A_rep = plogis(intercept) * 100
  ) %>%
  ggplot(aes(x = response_A_rep, y = reorder(vis_interact, desc(vis_order)))) +
    stat_slabh() +
    geom_vline(xintercept = 50, color = "red") +
    theme_minimal() +
    theme(
      axis.title = element_blank(),
      panel.grid.minor = element_blank()
    )

plt
ggsave("../../figures/components/e1-intercepts.svg", plt, width = 3, height = 2)
```

This view of the data helps us make sense of the magnitude of bias in the task. Users are over and underestimating the probability of a treatment effect by as much as 20% in some conditions. This is a very large amount of bias and was not necessarily something we expected to see (i.e., this was not even a preregistered comparison).


### Interactions of visualization with delta p and sample size

In addition to how LLO slopes vary as a function of visualization condition, we want to investigate what aspects of the signal in a chart users seems to struggle to interpret. *The signal in our task for experiment 1 can be broken down into two attributes of the stimulus: delta p and sample size.*

*Delta p* is the difference in the proportion of people in each data set with the disease depending on whether they did vs didn't receive the treatment. Negative values of delta p indicate that a greater proportion of people had the disease in the treatment group than in the no treatment group (i.e., evidence against treatment effectiveness). Positive values of delta p indicate that a smaller proportion of people had the disease if they received treatment than if they didn't (i.e., evidence for treatment effectiveness).

*Sample size* is just the overall number of people in the fake data sets we showed on each trial.

In the ideal observer, there should be no residual effects of delta p and sample size after we've adjusted for the influence of causal support on user judgments. However, users have perceptual and cognitive biases in interpreting charts, which result in residual effects of delta p and sample size on user's responses.

Here, we investigate preregistered comparisons of LLO slopes at different levels of delta p and sample size for each visualization condition. The degree to which LLO slopes deviate from one indicates how much these perceptual and cognitive biases distort sensitivity to the signal in charts.

#### Slopes

First, let's look at the *interaction between delta p and visualization condition on LLO slopes*. These lines should be flat with a y-intercept of 1 in an ideal observer.

```{r}
results_df %>%
  group_by(delta_p, vis_interact, vis_order, .draw) %>%  # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%            # marginalize
  ggplot(aes(x = delta_p, y = slope, group = .draw)) +
    geom_line(alpha = 0.1) +
    theme_bw() + 
    facet_grid(. ~ reorder(vis_interact, vis_order))
```

```{r eval=FALSE}
# figure for paper
plt <- results_df %>%
  group_by(delta_p, vis_interact, vis_order, .draw) %>%  # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%            # marginalize
  ggplot(aes(x = delta_p, y = slope, group = .draw)) +
    geom_line(alpha = 0.1) +
    theme_minimal() + 
    theme(
      axis.title = element_blank(),
      panel.grid.minor = element_blank(),
      strip.text = element_blank()
    ) +
    facet_grid(. ~ reorder(vis_interact, vis_order))

plt
ggsave("../../figures/components/e1-delta-p.svg", plt, width = 7, height = 1.5)
```

We can see that especially in the conditions with a tabular layout (i.e., aggbars, bars, icons, and text), users are more sensitive to signal (slopes closer to 1) in the charts when delta p is negative. This suggests that users are more sensitive to evidence against a treatment effect than evidence for one. Interestingly, this pattern seems to diminish when users interact with aggbars, with less sensitively at negative delta p, which may help to explain poorer performance when users interact with aggbars. Similarly, this pattern does not seem to happen much with filtbars, where sensitivity is much more uncertain at negative delta p.

Now, let's look at the *interaction between sample size and visualization condition on LLO slopes*.

```{r}
results_df %>%
  group_by(n, vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%     # marginalize
  ggplot(aes(x = n, y = slope)) +
    stat_halfeye() +
    theme_bw() +
    facet_grid(. ~ reorder(vis_interact, vis_order))
```

```{r eval=FALSE}
# figure for paper
plt <- results_df %>%
  group_by(n, vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%     # marginalize
  ggplot(aes(x = n, y = slope)) +
    stat_slab() +
    theme_minimal() + 
    theme(
      axis.title = element_blank(),
      panel.grid.minor = element_blank(),
      strip.text = element_blank()
    ) +
    facet_grid(. ~ reorder(vis_interact, vis_order))

plt
ggsave("../../figures/components/e1-n.svg", plt, width = 7, height = 1.5)
```

We see that users are less sensitive to the signal in charts as sample size increases, with the exception filtbars where performance is poor across the board. This trend is also less pronounced in the text condition. In particular, users seem to do best at low sample size, perhaps because the relatively small amount of data is a cue that strong inferences are not warranted. Although, this pattern may be more of a perceptual bias than a cognitive one. This result is consistent with prior work showing that people underestimate the number of items in a set and underestimate sample size for the purpose of making visual inferences with data.

#### Intercepts

Although we did not preregister comparisons looking at the *interaction of visualization, delta p, and sample size on LLO intercepts*, the substantial amount of bias we saw in the intercept estimates per visualization condition make us curious.

We'll start by looking at the *interaction of delta p and visualization on LLO intercepts*. Astute readers will note that this is somewhat of a nonsensical counterfactual insofar as causal support depends in part on delta p, and extreme values of delta p seldom occur when ground truth causal support is 0 (i.e., at the intercept), with the exception of very very small sample sizes.

```{r}
results_df %>%
  group_by(delta_p, vis_interact, vis_order, .draw) %>% # group by predictors to keep
  summarise(intercept = weighted.mean(intercept)) %>%   # marginalize
  ggplot(aes(x = delta_p, y = intercept, group = .draw)) +
    geom_line(alpha = 0.1) +
    theme_bw() + 
    facet_grid(. ~ reorder(vis_interact, vis_order))
```

Here, we see what we expect to see if people understand the task. All else being equal, users say the treatment effect is more likely when the difference in the proportion of people with the disease suggests an effective treatment. This is a nice sanity check more than anything else.

Now, we'll look at the *interaction of sample size and visualization on LLO intercepts*. This query makes more sense than the last one since we can imagine scenarios at any sample size where the evidence for a treatment effect would appear totally ambiguous (i.e., ground truth causal support = 0).

```{r}
results_df %>%
  group_by(n, vis_interact, vis_order, .draw) %>%     # group by predictors to keep
  summarise(intercept = weighted.mean(intercept)) %>% # marginalize
  ggplot(aes(x = n, y = intercept)) +
    stat_halfeye() +
    theme_bw() +
    facet_grid(. ~ reorder(vis_interact, vis_order))
```

We can see that users tend to be the least biased in their responses at small sample sizes, especially with icons, bars, and aggbars. This bias to underestimate the probability of a treatment effect more at larger sample sizes is peculiar and unexpected but clearly a robust pattern.


### User interactions with aggbars and filtbars

Let's analyze how users interacted with the aggbars and filtbars visualization conditions, respectively. 

We'll start by writing functions to reconstruct the state of each visualization on each trial based on interaction logs.

```{r}
reconstruct_state_aggbars <- function(interactions) {
  # starting state is conditioning on both gene and treatment
  states <- list("gene_treat_init")
  
  for(i in 1:length(interactions)) {
    if (interactions[i] == "collapseRow" & str_detect(states[length(states)], "^gene_treat")) {
      states <- append(states, list("treat"))
    } else if (interactions[i] == "collapseRow" & states[length(states)] == "gene") {
      states <- append(states, list("none"))
    } else if (interactions[i] == "expandRow" & states[length(states)] == "treat") {
      states <- append(states, list("gene_treat"))
    } else if (interactions[i] == "expandRow" & states[length(states)] == "none") {
      states <- append(states, list("gene"))
    } else if (interactions[i] == "collapseCol" & str_detect(states[length(states)], "^gene_treat")) {
      states <- append(states, list("gene"))
    } else if (interactions[i] == "collapseCol" & states[length(states)] == "treat") {
      states <- append(states, list("none"))
    } else if (interactions[i] == "expandCol" & states[length(states)] == "gene") {
      states <- append(states, list("gene_treat"))
    } else if (interactions[i] == "expandCol" & states[length(states)] == "none") {
      states <- append(states, list("treat"))
    } 
  }
  
  return(unlist(states))
}
```

```{r}
reconstruct_state_filtbars <- function(interactions) {
  # starting state is conditioning on nothing
  states <- list("none_init")
  curr <- "" # state is a chain of filters
  
  for(i in 1:length(interactions)) {
    if (interactions[i] == "clearFilters") {
      curr <- ""
      states <- append(states, list("none"))
    } else if (str_detect(interactions[i], "^filter") & str_detect(states[length(states)], "^none")) {
      # first filter
      curr <- sub("^filter", "", interactions[i])
      states <- append(states, list(curr))
    } else if (str_detect(interactions[i], "^filter") & !str_detect(curr, paste(".*", sub("^filter", "", interactions[i]), ".*", sep = ""))) { 
      # only add interactions not already in the chain (don't log duplicate filters which do not change the state)
      # put chain of filters including the current one into consistent order (so string matching can identify unique states)
      curr <- pmap_chr(list(curr, sub("^filter", "", interactions[i])), ~paste(sort(c(...)), collapse = "_"))
      states <- append(states, list(curr))
    }
  }
  
  return(unlist(states))
}
```

Now, we'll reconstruct the states visited on each trial for each visualization separately.

```{r}
aggbars_df <- model_df %>% 
  filter(condition == "aggbars") %>%
  rowwise() %>%
  mutate(
    interactions = str_split(interactions, "_"),
    state = list(reconstruct_state_aggbars(interactions))
  )
```

```{r}
filtbars_df <- model_df %>% 
  filter(condition == "filtbars") %>%
  rowwise() %>%
  mutate(
    interactions = str_split(interactions, "_"),
    state = list(reconstruct_state_filtbars(interactions))
  )
```

Let's view a histogram of the *states visited by users of aggbars*. These are named according to the conditions that users applied to the data in order to (dis)aggregate it.

```{r}
aggbars_df %>%
  group_by(trial, workerId) %>%
  unnest(cols = c("state")) %>%
  ggplot(aes(y = state)) +
  geom_bar() +
  theme_bw()
```

We can see that aggbars users create views conditioning on gene about as much as they create views conditioning on treatment.

Now, let's view a histogram of the *states visited by users of filtbars*. These are named according to the conditions that users applied to the data in order to filter it.

```{r}
filtbars_df %>%
  group_by(trial, workerId) %>%
  unnest(cols = c("state")) %>%
  ggplot(aes(y = state)) +
  geom_bar() +
  theme_bw()
```

We can see that users of filtbars are more likely to condition on treatment, if they interact with the visualization at all. Interestingly, some users also click the disease bar. While conditioning on the outcome variable is not a statistically valid one, it is a quick and intuitive way to see what other factors are most associated with getting the disease.

For both aggbars and filtbars, let's see *what proportion of users create views that should be most helpful for the task*.

For aggbars this means intentionally creating views that condition on treatment, which we see in about 24% of trials.

```{r}
aggbars_df <- aggbars_df %>% 
  mutate(
    condition_on_treat = any(str_detect(unlist(state), ".*treat$"))
  )

sum(aggbars_df$condition_on_treat) / length(aggbars_df$condition_on_treat)
```

For filtbars this means intentionally creating views that condition on both treatment and no treatment, which we see in about 33% of trials.

```{r}
filtbars_df <- filtbars_df %>% 
  mutate(
    condition_on_treat = any(str_detect(unlist(state), "^Treatment$")) & any(str_detect(unlist(state), "^NoTreatment$"))
  )

sum(filtbars_df$condition_on_treat) / length(filtbars_df$condition_on_treat)
```

The fact that people who interact with aggbars focus less on task relevant views than people who interact with filtbars may help to explain why interacting with aggbars is associated with worse performance while interacting with filtbars is associated with better performance. 

Also, baseline performance with filtbars when people don't interact is horrible, whereas baseline performance with aggbars when people don't interact is decent. These different baselines may have something to do with the opposite directions of effect of interacting with the visualizations, since performance with filtbars could really only improve from baseline.
