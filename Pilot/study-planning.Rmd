---
title: "Causal Support: Stimuli and Task Structure"
author: "Alex Kale"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggdist)
library(RColorBrewer)
```

## Overview: Causal support as benchmark for causal induction

The core idea of this study is to establish *causal support*, as defined by [Griffiths and Tenenbaum](https://web.mit.edu/cocosci/Papers/structure-strength-reprint.pdf), as a normative benchmark for causal induction tasks with visualizations, where chart users attempt to reason about the causal structure underlying data.

Following Griffiths and Tenenbaum, we will present participants with scenarios involving two alternative causal explanations for data, represented by directed acyclic graphs (DAGs), and we will ask them to say the probability that one of those graphs vs the other generated a given data set. We will evaluate these elicited probabilities by comparing them to *causal support*, which is the log likelihood ratio of the data given the two alternative models. While this can be extended to use cases with more than two models, we will focus on toy problems for causal induction to learn what we can as a proof of concept for causal support as an evaluation strategy for causal reasoning with visualizations.

In this document, we plan the data and alternative models we will show participants. Our aim is to plan a trial structure and the copy for a crowdsourced experiment. Adapting tasks from Griffiths and Tenenbaum, as well as Wu, we will ask participants to look at two-way tables showing counts of people with and without cancer, given the presence or absence of a gene and the presence or absence of immunotherapy. On each trial we, will show a fake data set and ask for a probability judgment.


## Problem 1: Does immunotherapy reduce the rate of cancer? 

The copy for this problem will be something like: *"Scientists have found a gene that is known to increase the probability of developing a particular type of cancer. In recent years, scientists have developed an experimental immunotherapy which may reduce the probability of developing this cancer. These data show the number of patients who do and do not develop cancer grouped based on whether or not they have the gene known to cause this cancer and whether or not they have received immunotherapy. Based on the data in the table, what is the probability that the immunotherapy is effective at reducing cancer?"*

Before we get into generating fake data, we need a way of evaluating the ground truth probability that the immunotherapy is effective. 

### Monte Carlo simulations

To operationalize a normative judgment on this task, we set up a Monte Carlo simulation for each alternative DAG or hypothesis about causal structure. We represent each DAG as a parameterized simulation where we calculate the log likelihood of the data by integrating over a uniform prior on parameter values. Note that unlike Griffiths and Tenenbaum, we are assuming that there is a base rate of the effect (cancer) that is not explained by the covariate (gene).

```{r}
# expects data as counts from a two-way table formatted as follows:
#   A   - count with cancer given gene T and immunotherapy T
#   nA  - total given gene T and immunotherapy T
#   B   - count with cancer given gene F and immunotherapy T
#   nB  - total given gene F and immunotherapy T
#   C   - count with cancer given gene T and immunotherapy F
#   nC  - total given gene T and immunotherapy F
#   D   - count with cancer given gene F and immunotherapy F
#   nD  - total given gene F and immunotherapy F
# the boolean immunotherapy_works determines which DAG is simulated
likelihood_from_monte_carlo <- function (A, nA, B, nB, C, nC, D, nD, immunotherapy_works = TRUE) {
  # define parameters (sample from uniform distribution)
  m <- 10000             # number of simulations
  p_cancer <- runif(m)   # base rate of cancer due to unknown causes
  p_gene <- runif(m)     # probability that gene causes cancer
  if (immunotherapy_works) {
    p_immun <- runif(m)  # probability that immunotherapy prevents cancer
  } else {
    p_immun <- rep(0, m) # immunotherapy has no effect on cancer
  }
  
  
  # calculate counts for each possible outcome in the contingengy table
  counts <- cbind(
    A,
    nA - A,
    B,
    nB - B,
    C,
    nC - C,
    D,
    nD - D
  )
  
  # calculate probabilies for each possible outcome in the contingency table
  probs <- cbind(
    (p_gene + p_cancer - p_gene*p_cancer)*(1 - p_immun),                         # p(cancer|gene T, immunotherapy T)
    (1 - p_gene)*(1 - p_cancer) + (p_gene + p_cancer - p_gene*p_cancer)*p_immun, # p(~cancer|gene T, immunotherapy T)
    p_cancer*(1 - p_immun),                                                      # p(cancer|gene F, immunotherapy T)
    (1 - p_cancer) + p_cancer*p_immun,                                           # p(~cancer|gene F, immunotherapy T)
    p_gene + p_cancer - p_gene*p_cancer,                                         # p(cancer|gene T, immunotherapy F)
    (1 - p_gene)*(1 - p_cancer),                                                 # p(~cancer|gene T, immunotherapy F)
    p_cancer,                                                                    # p(cancer|gene F, immunotherapy F)
    (1 - p_cancer)                                                               # p(~cancer|gene F, immunotherapy F)
  )
  
  # calculate log likelihood of data for each of m runs of the simulation
  loglik <- rowSums(matrix(rep(counts, m), nrow = m, ncol = 8, byrow = TRUE) * log(probs))
  # normalize by maximum likelihood to make probabilities comparable across Monte Carlo simulations
  # and marginalize over simulated parameter values
  logmax <- max(loglik)
  logscore <- logmax + log(sum(exp(loglik - logmax))) - log(m)
}
```

Now we can generate fake data sets and calculate how much causal support there is for the effectiveness of the immunotherapy. This enables to establish a normative benchmark for responses.

### Exploring the design space of fake data for stimuli

We're actually going to use the same simulations of data generating processes above to generate a bunch of possible stimuli. We'll assume range of setups which we might expect to influence task difficulty, namely:
 
 - the probability of cancer given the gene, 
 - the base rate for the gene, 
 - the probability that immunotherapy prevents cancer, 
 - the base rate for immunotherapy
 - the probability of cancer due to unexamined causes
 - sample size

This will help us get a sense of the design space for stimuli, so we can choose which of these parameters we want to hold constant or manipulate across trials. For now, just to prevent the simulation from getting too large, we'll assume that these parameters are perfectly represented in the fake data (i.e., we are not simulated sampling error yet).

Here's our fake data.

```{r}
param_space_df <- expand.grid(
    p_gene = seq(from = 0.1, to = 0.7, by = 0.2),                      # gene effect
    br_gene = seq(from = 0.05, to = 0.5, by = 0.15),                   # base rate of gene
    p_immun = seq(from = 0, to = 0.9, by = 0.15),                      # immunotherapy effect 
    br_immun = seq(from = 0.05, to = 0.5, by = 0.15),                  # base rate of immunotherapy
    p_cancer = c(0.01, 0.05, 0.1),                                     # base rate of cancer
    N = c(100, 5000)                                                   # small and large sample size
  ) %>%
  mutate(
    nA = round(N*br_gene*br_immun),                                    # N(gene T, immunotherapy T)
    A = round(nA*(p_gene + p_cancer - p_gene*p_cancer)*(1 - p_immun)), # N(cancer|gene T, immunotherapy T)
    nB = round(N*(1 - br_gene)*br_immun),                              # N(gene F, immunotherapy T)
    B = round(nB*p_cancer*(1 - p_immun)),                              # N(cancer|gene F, immunotherapy T)
    nC = round(N*br_gene*(1 - br_immun)),                              # N(gene T, immunotherapy F)
    C = round(nC*(p_gene + p_cancer - p_gene*p_cancer)),               # N(cancer|gene T, immunotherapy F)
    nD = round(N*(1 - br_gene)*(1 - br_immun)),                        # N(gene F, immunotherapy F)
    D = round(nD*p_cancer)                                             # N(cancer|gene F, immunotherapy F)
  )

head(param_space_df)
```

Now, let's apply our version of Griffiths and Tenenbaum's causal support calculation to the data in each row, where each row represents a possible data generating process without noise.

```{r}
param_space_df = param_space_df %>%
  rowwise() %>%
  mutate(
    # calculate causal support for each row difference of log likelihoods is a log odds ratio
    causal_support = likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, TRUE) - likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, FALSE), 
    # convert causal support into normative probability judgment
    ground_truth = plogis(causal_support)
  )
```

Let's visualize the ground truth suggested by causal support as a function of some of these manipulations.

To start, let's see how sample size and the base rates of the gene and immunotherapy impact the ability to detect various effects of immunotherapy, holding the effect of the gene and the base rate of cancer constant.

```{r}
param_space_df %>% 
  filter(p_gene == 0.1, p_cancer == 0.01) %>%
  ggplot(aes(x = p_immun, y = ground_truth, color = as.factor(N))) +
  geom_line() +
  theme_bw() +
  facet_grid(br_gene ~ br_immun)
```

We can see that the normative response is more certain (closer to 0 or 1) when sample size is high (teal line), regardless of other manipulations. This is consistent with what we might expect. *Manipulating sample size is a good way to manipulate task difficulty.* 

Additionally, *manipulating the effect of the immunotherapy is an obvious way to change the task difficulty.*

We can also see that the normative response gets more certain when the base rate of immunotherapy (columns) is high. This also makes intuitive sense because the more people we give the immunotherapy to, the more information we have to assess its effectiveness. *We may want to hold the base rate of immunotherapy constant by always presenting a balanced sample.*

It seems like the base rate of the gene (rows) has less impact on task difficulty, except when the effect of immunotherapy (x-axis) is small, suggesting that *maybe we want to hold the base rate of the gene constant.*

Now, let's modify the query above to see how the sample size, the effect of the gene, and the base rate of cancer impact the ability to detect various effects of immunotherapy, holding the base rates of the gene and immunotherapy constant.

```{r}
param_space_df %>% 
  filter(round(10*br_gene)/10 == 0.2, round(10*br_immun)/10 == 0.5) %>%
  ggplot(aes(x = p_immun, y = ground_truth, color = as.factor(N))) +
  geom_line() +
  theme_bw() +
  facet_grid(p_gene ~ p_cancer)
```

We can see that the normative response may be slightly more certain when the effect of the gene (rows) is larger, but the impacts are small and don't seem to be monotonic. Griffiths and Tenenbaum talk about this as a unique prediction of causal support compared to other models of causal induction. *We would probably only want to manipulate the effect of the gene if we think that predicting this monotonacity is important to our findings.*

We can also see that the normative response is hardly impacted by the base rate of cancer due to unexplained causes (columns). *Maybe we want to hold the base rate of cancer constant.* 

#### Summary of data manipulations

Manipulating the sample size and the effect of immunotherapy seem like good ways to change task difficulty across trials.

To keep things simple, we probably want to present a balanced sample on each trial (i.e., an equal number of people who have vs haven't received the immunotherapy).

Above, I've suggested that we could probably hold the following variables constant: base rate of the gene, effect of the gene, and base rate of cancer. However, if we want to sell the idea that each trial is asking about a different gene and type of cancer, we might want to add a small amount of random noise to these fixed values on each trial. Adding sampling error to our stimulus generation process will achieve this. These small variations won't impact causal support very much, and they might make the task scenario more believable.

Additionally, it might be interesting to test low and high values for the base rate of the gene to see how judgments of intervention effectiveness change as a function of the prevalence of risk. Maybe instead of controlling the proportion of people that have the gene, we should pick low and high values and shuffle them across trials as an exploratory manipulation. *We ended up deciding not to do this since it seems more relevant to a decision-making task than causal reasoning.*

### Trial structure

Based on the insights above, lets generate a bunch of stimuli based on a narrower set of parameters. Now we'll add sampling error by drawing events from a binomial distribution, except in the case of the base rate of immunotherapy because we will always present a balanced sample. To be realistic, we'll use the actual base rate of cancer in the US in our model, which is lower than any of the values above. 
<!-- We will sample the effectiveness of immunotherapy in preventing cancer linearly in log odds units, since this will likely result in a more perceptually uniform sampling of causal support. p_immun = plogis(seq(from = qlogis(0.01), to = qlogis(0.9), length.out = 4)) -->

```{r}
# number of simulated data sets per combination of factors
n_sims <- 2000

# set up manipulations and simulate fake data
stim_df <- expand.grid(
    p_gene = 0.3,                    # gene effect (control)
    br_gene = 0.2,                   # base rate of gene (control)
    p_immun = c(0, 0.25, 0.5, 0.75), # immunotherapy effect (4 levels)
    br_immun = 0.5,                  # base rate of immunotherapy (control, no noise)
    p_cancer = 0.004424,             # actual base rate of cancer (control)
    N = c(100, 5000)                 # sample size (small and large)
  ) %>%
  rowwise() %>%
  mutate(
    # possible numbers of people in each cell of the two-way table
    draw = list(seq(from = 1, to = n_sims, by = 1)),
    nA = list(rbinom(n_sims, round(N*br_immun), br_gene)),            # N(gene T, immunotherapy T)
    nB = list(rbinom(n_sims, round(N*br_immun), (1 - br_gene))),      # N(gene F, immunotherapy T)
    nC = list(rbinom(n_sims, round(N*(1 - br_immun)), br_gene)),      # N(gene T, immunotherapy F)
    nD = list(rbinom(n_sims, round(N*(1 - br_immun)), (1 - br_gene))) # N(gene F, immunotherapy F)
  ) %>%
  unnest(cols = c("draw", "nA", "nB", "nC", "nD")) %>%
  rowwise() %>%
  mutate(
    A = rbinom(1, nA, (p_gene + p_cancer - p_gene*p_cancer)*(1 - p_immun)), # N(cancer|gene T, immunotherapy T)
    B = rbinom(1, nB, p_cancer*(1 - p_immun)),                              # N(cancer|gene F, immunotherapy T)
    C = rbinom(1, nC, (p_gene + p_cancer - p_gene*p_cancer)),               # N(cancer|gene T, immunotherapy F)
    D = rbinom(1, nD, p_cancer)                                             # N(cancer|gene F, immunotherapy F)
  )

head(stim_df)
```

Now, let's calculate causal support for each simulated data set.

```{r}
stim_df = stim_df %>%
  rowwise() %>%
  mutate(
    # calculate causal support for each row difference of log likelihoods is a log odds ratio
    causal_support = likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, TRUE) - likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, FALSE), 
    # convert causal support into normative probability judgment
    ground_truth = plogis(causal_support)
  )
```

Now, let's visualize the ground truth probability that immunotherapy works, which is derived from causal support, as a function of our manipulations to see how difficult our task setup is.

We can look at the trend across trials as a line as before, but now with uncertainty due to simulated sampling error.

```{r}
stim_df %>% 
  ggplot(aes(x = p_immun, y = ground_truth)) +
  stat_lineribbon(.width = c(0.95, 0.8, 0.5), color = "black") +
  scale_fill_brewer() +
  theme_bw() +
  facet_grid(N ~ .)
```

Alternatively, we can look at quantile dotplots of simulated trials.

```{r}
stim_df %>% 
  ggplot(aes(x = as.factor(p_immun), y = ground_truth)) +
  stat_dots(position = "dodge", binwidth = 0.04, quantiles = 20) +
  theme_bw() +
  facet_grid(N ~ .)
```

In both charts, we can see that we are sampling a nice range of the ground truth with this trial structure.

#### Study design

For our first experiment, I propose to show eight trials of problem 1 such that we test each combination of sample size (2 levels) and immunotherapy effectiveness (4 levels). On each trial, one of the simulated data sets that we generated above will be selected at random.

This design should result in data that are realistically noisy and with rates of cancer that vary enough from trial to trial that it is believable that we are talking about a different gene, cancer, and immunotherapy on each trial. We will get balanced measurements of the impact of sample size and immunotherapy effectiveness on judgments.

Assuming causal support as a normative model, we hope to learn about the ways that causal reasoning with visualizations deviate from this norm. We will explore the following research questions by gradually adding different predictors to a model predicting human judgments from causal support:

- Does underestimating sample size, as found in prior work, result in perceived causal support which is less than the normative value (i.e., probabilities closed to 0.5)? Does the effect of sample size that is not already explained by normative causal support depend on the data presentation format?
- Do people under- or over-estimate intervention effectiveness in a way that biases their causal support judgments? Underestimation would result in probabilties that are too low and overestimation would result in probabilities that are too high. Does this pattern of bias, if it exists, depend on data presentation format?
- How does data presentation format itself influence causal induction? Is there a greater bias in some conditions? Are people's judgments more or less variable in some conditions? Specifically, how are judgments different when we show data in a *numerical table* vs a *table of icon arrays* vs a *table where the user can interactively (dis)aggregate the data by expanding or collapsing dimensions of the table* (emulating controlling for vs ignoring levels of a factor)?


## Problem 2: Does the gene that causes cancer also reduce the effectiveness of immunotherapy?

The copy for this problem will be something like: *"Scientists have found a gene that is known to increase the probability of developing a particular type of cancer. Scientists have also developed an immunotherapy which is known to reduce the probability of developing this cancer. However, in recent years, some scientists have hypothesized that the same gene known to cause cancer may also reduce the effectiveness of the immunotherapy. These data show the number of patients who do and do not develop cancer grouped based on whether or not they have the gene known to cause this cancer and whether or not they have received immunotherapy. Based on the data in the table, what is the probability that the gene reduces the is effectiveness of the immunotherapy?"*

### Monte Carlo simulation

We'll need to set up an additional Monte Carlo simulation to represent the case where there is a confounding effect of the gene on an immunotherapy that is known to be effective...

