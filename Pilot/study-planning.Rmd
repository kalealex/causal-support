---
title: "Causal Support: Stimuli and Task Structure"
author: "Alex Kale"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(ggdist)
library(RColorBrewer)
set.seed(1234)
```

## Overview: Causal support as benchmark for causal induction

The core idea of this study is to establish *causal support*, as defined by [Griffiths and Tenenbaum](https://web.mit.edu/cocosci/Papers/structure-strength-reprint.pdf), as a normative benchmark for causal induction tasks with visualizations, where chart users attempt to reason about the causal structure underlying data.

Following Griffiths and Tenenbaum, we will present participants with scenarios involving two alternative causal explanations for data, represented by directed acyclic graphs (DAGs), and we will ask them to say the probability that one of those graphs vs the other generated a given data set. We will evaluate these elicited probabilities by comparing them to *causal support*, which is the log likelihood ratio of the data given the two alternative models. While this can be extended to use cases with more than two models, we will focus on toy problems for causal induction to learn what we can as a proof of concept for causal support as an evaluation strategy for causal reasoning with visualizations.

In this document, we plan the data and alternative models we will show participants. Our aim is to plan a trial structure and the copy for a crowdsourced experiment. Adapting tasks from Griffiths and Tenenbaum, as well as Wu, we will ask participants to look at two-way tables showing counts of people with and without cancer, given the presence or absence of a gene and the presence or absence of immunotherapy. On each trial we, will show a fake data set and ask for a probability judgment.


## Problem 1: Does immunotherapy reduce the rate of cancer? 

The copy for this problem will be something like: *"Scientists have found a gene that is known to increase the probability of developing a particular type of cancer. In recent years, scientists have developed an experimental immunotherapy which may reduce the probability of developing this cancer. These data show the number of patients who do and do not develop cancer grouped based on whether or not they have the gene known to cause this cancer and whether or not they have received immunotherapy. Based on the data in the table, what is the probability that the immunotherapy is effective at reducing cancer?"*

Before we get into generating fake data, we need a way of evaluating the ground truth probability that the immunotherapy is effective. 

### Monte Carlo simulations

To operationalize a normative judgment on this task, we set up a Monte Carlo simulation for each alternative DAG or hypothesis about causal structure. We represent each DAG as a parameterized simulation where we calculate the log likelihood of the data by integrating over a uniform prior on parameter values. Note that unlike Griffiths and Tenenbaum, we are assuming that there is a base rate of the effect (cancer) that is not explained by the covariate (gene).

```{r}
# expects data as counts from a two-way table formatted as follows:
#   A   - count with cancer given gene T and immunotherapy T
#   nA  - total given gene T and immunotherapy T
#   B   - count with cancer given gene F and immunotherapy T
#   nB  - total given gene F and immunotherapy T
#   C   - count with cancer given gene T and immunotherapy F
#   nC  - total given gene T and immunotherapy F
#   D   - count with cancer given gene F and immunotherapy F
#   nD  - total given gene F and immunotherapy F
# the boolean immunotherapy_works determines which DAG is simulated
likelihood_from_monte_carlo <- function (A, nA, B, nB, C, nC, D, nD, immunotherapy_works = TRUE) {
  # define parameters (sample from uniform distribution)
  m <- 10000             # number of simulations
  p_cancer <- runif(m)   # base rate of cancer due to unknown causes
  p_gene_c <- runif(m)   # probability that gene causes cancer
  if (immunotherapy_works) {
    p_immun <- runif(m)  # probability that immunotherapy prevents cancer
  } else {
    p_immun <- rep(0, m) # immunotherapy has no effect on cancer
  }
  
  
  # calculate counts for each possible outcome in the contingengy table
  counts <- cbind(
    A,
    nA - A,
    B,
    nB - B,
    C,
    nC - C,
    D,
    nD - D
  )
  
  # calculate probabilies for each possible outcome in the contingency table
  probs <- cbind(
    (p_gene_c + p_cancer - p_gene_c*p_cancer)*(1 - p_immun),                           # p(cancer|gene T, immunotherapy T)
    (1 - p_gene_c)*(1 - p_cancer) + (p_gene_c + p_cancer - p_gene_c*p_cancer)*p_immun, # p(~cancer|gene T, immunotherapy T)
    p_cancer*(1 - p_immun),                                                            # p(cancer|gene F, immunotherapy T)
    (1 - p_cancer) + p_cancer*p_immun,                                                 # p(~cancer|gene F, immunotherapy T)
    p_gene_c + p_cancer - p_gene_c*p_cancer,                                           # p(cancer|gene T, immunotherapy F)
    (1 - p_gene_c)*(1 - p_cancer),                                                     # p(~cancer|gene T, immunotherapy F)
    p_cancer,                                                                          # p(cancer|gene F, immunotherapy F)
    (1 - p_cancer)                                                                     # p(~cancer|gene F, immunotherapy F)
  )
  
  # calculate log likelihood of data for each of m runs of the simulation
  loglik <- rowSums(matrix(rep(counts, m), nrow = m, ncol = 8, byrow = TRUE) * log(probs))
  # normalize by maximum likelihood to make probabilities comparable across Monte Carlo simulations
  # and marginalize over simulated parameter values
  logmax <- max(loglik)
  logscore <- logmax + log(sum(exp(loglik - logmax))) - log(m)
}
```

Now we can generate fake data sets and calculate how much causal support there is for the effectiveness of the immunotherapy. This enables to establish a normative benchmark for responses.

### Exploring the design space of fake data for stimuli

We're actually going to use the same simulations of data generating processes above to generate a bunch of possible stimuli. We'll assume range of setups which we might expect to influence task difficulty, namely:
 
 - the probability of cancer given the gene, 
 - the base rate for the gene, 
 - the probability that immunotherapy prevents cancer, 
 - the base rate for immunotherapy
 - the probability of cancer due to unexamined causes
 - sample size

This will help us get a sense of the design space for stimuli, so we can choose which of these parameters we want to hold constant or manipulate across trials. For now, just to prevent the simulation from getting too large, we'll assume that these parameters are perfectly represented in the fake data (i.e., we are not simulated sampling error yet).

Here's our fake data.

```{r}
param_space_df <- expand.grid(
    p_gene_c = seq(from = 0.1, to = 0.7, by = 0.2),                    # gene effect on cancer
    br_gene = seq(from = 0.05, to = 0.5, by = 0.15),                   # base rate of gene
    p_immun = seq(from = 0, to = 0.9, by = 0.15),                      # immunotherapy effect 
    br_immun = seq(from = 0.05, to = 0.5, by = 0.15),                  # base rate of immunotherapy
    p_cancer = c(0.01, 0.05, 0.1),                                     # base rate of cancer
    N = c(100, 5000)                                                   # small and large sample size
  ) %>%
  mutate(
    nA = round(N*br_gene*br_immun),                                        # N(gene T, immunotherapy T)
    A = round(nA*(p_gene_c + p_cancer - p_gene_c*p_cancer)*(1 - p_immun)), # N(cancer|gene T, immunotherapy T)
    nB = round(N*(1 - br_gene)*br_immun),                                  # N(gene F, immunotherapy T)
    B = round(nB*p_cancer*(1 - p_immun)),                                  # N(cancer|gene F, immunotherapy T)
    nC = round(N*br_gene*(1 - br_immun)),                                  # N(gene T, immunotherapy F)
    C = round(nC*(p_gene_c + p_cancer - p_gene_c*p_cancer)),               # N(cancer|gene T, immunotherapy F)
    nD = round(N*(1 - br_gene)*(1 - br_immun)),                            # N(gene F, immunotherapy F)
    D = round(nD*p_cancer)                                                 # N(cancer|gene F, immunotherapy F)
  )

head(param_space_df)
```

Now, let's apply our version of Griffiths and Tenenbaum's causal support calculation to the data in each row, where each row represents a possible data generating process without noise.

```{r}
param_space_df = param_space_df %>%
  rowwise() %>%
  mutate(
    # calculate causal support for each row difference of log likelihoods is a log odds ratio
    causal_support = likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, TRUE) - likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, FALSE), 
    # convert causal support into normative probability judgment
    ground_truth = plogis(causal_support)
  )
```

Let's visualize the ground truth suggested by causal support as a function of some of these manipulations.

To start, let's see how sample size and the base rates of the gene and immunotherapy impact the ability to detect various effects of immunotherapy, holding the effect of the gene and the base rate of cancer constant.

```{r}
param_space_df %>% 
  filter(p_gene_c == 0.1, p_cancer == 0.01) %>%
  ggplot(aes(x = p_immun, y = ground_truth, color = as.factor(N))) +
  geom_line() +
  theme_bw() +
  facet_grid(br_gene ~ br_immun)
```

We can see that the normative response is more certain (closer to 0 or 1) when sample size is high (teal line), regardless of other manipulations. This is consistent with what we might expect. *Manipulating sample size is a good way to manipulate task difficulty.* 

Additionally, *manipulating the effect of the immunotherapy is an obvious way to change the task difficulty.*

We can also see that the normative response gets more certain when the base rate of immunotherapy (columns) is high. This also makes intuitive sense because the more people we give the immunotherapy to, the more information we have to assess its effectiveness. *We may want to hold the base rate of immunotherapy constant by always presenting a balanced sample.*

It seems like the base rate of the gene (rows) has less impact on task difficulty, except when the effect of immunotherapy (x-axis) is small, suggesting that *maybe we want to hold the base rate of the gene constant.*

Now, let's modify the query above to see how the sample size, the effect of the gene, and the base rate of cancer impact the ability to detect various effects of immunotherapy, holding the base rates of the gene and immunotherapy constant.

```{r}
param_space_df %>% 
  filter(round(10*br_gene)/10 == 0.2, round(10*br_immun)/10 == 0.5) %>%
  ggplot(aes(x = p_immun, y = ground_truth, color = as.factor(N))) +
  geom_line() +
  theme_bw() +
  facet_grid(p_gene_c ~ p_cancer)
```

We can see that the normative response may be slightly more certain when the effect of the gene (rows) is larger, but the impacts are small and don't seem to be monotonic. Griffiths and Tenenbaum talk about this as a unique prediction of causal support compared to other models of causal induction. *We would probably only want to manipulate the effect of the gene if we think that predicting this monotonacity is important to our findings.*

We can also see that the normative response is hardly impacted by the base rate of cancer due to unexplained causes (columns). *Maybe we want to hold the base rate of cancer constant.* 

#### Summary of data manipulations

Manipulating the sample size and the effect of immunotherapy seem like good ways to change task difficulty across trials.

To keep things simple, we probably want to present a balanced sample on each trial (i.e., an equal number of people who have vs haven't received the immunotherapy).

Above, I've suggested that we could probably hold the following variables constant: base rate of the gene, effect of the gene, and base rate of cancer. However, if we want to sell the idea that each trial is asking about a different gene and type of cancer, we might want to add a small amount of random noise to these fixed values on each trial. Adding sampling error to our stimulus generation process will achieve this. These small variations won't impact causal support very much, and they might make the task scenario more believable.

Additionally, it might be interesting to test low and high values for the base rate of the gene to see how judgments of intervention effectiveness change as a function of the prevalence of risk. Maybe instead of controlling the proportion of people that have the gene, we should pick low and high values and shuffle them across trials as an exploratory manipulation. *We ended up deciding not to do this since it seems more relevant to a decision-making task than causal reasoning.*

### Trial structure

Based on the insights above, lets generate a bunch of stimuli based on a narrower set of parameters. Now we'll add sampling error by drawing events from a binomial distribution, except in the case of the base rate of immunotherapy because we will always present a balanced sample. To be realistic, we'll use the actual base rate of cancer in the US in our model, which is lower than any of the values above. 

```{r}
# number of simulated data sets per combination of factors
n_sims <- 2000

# set up manipulations and simulate fake data
stim1_df <- expand.grid(
    p_gene_c = 0.3,                  # gene effect on cancer (control)
    br_gene = 0.4,                   # base rate of gene (control)
    p_immun = c(0, 0.1, 0.3, 0.4),   # immunotherapy effect (4 levels)
    br_immun = 0.5,                  # base rate of immunotherapy (control, no noise)
    p_cancer = 0.004424,             # actual base rate of cancer (control)
    N = c(100, 500, 1000, 5000)      # sample size (4 levels)
  ) %>%
  rowid_to_column("trialIdx") %>%
  rowwise() %>%
  mutate(
    # get a balanced sample of people do and do not receive treatment
    nTreat = round(N*br_immun),  
    nNoTreat = round(N*(1 - br_immun)),
    # sample possible numbers of people in each group who have the gene
    draw = list(seq(from = 1, to = n_sims, by = 1)),
    nA = list(rbinom(n_sims, nTreat, br_gene)),      # N(gene T, immunotherapy T)
    nC = list(rbinom(n_sims, nNoTreat, br_gene)) # N(gene T, immunotherapy F)
  ) %>%
  unnest(cols = c("draw", "nA", "nC")) %>%
  mutate(
    # calculate number of people in each group who DO NOT have the gene for each draw
    nB = nTreat - nA,  # N(gene F, immunotherapy T)
    nD = nNoTreat - nC # N(gene F, immunotherapy F)
  ) %>%
  rowwise() %>%
  mutate(
    A = rbinom(1, nA, (p_gene_c + p_cancer - p_gene_c*p_cancer)*(1 - p_immun)), # N(cancer|gene T, immunotherapy T)
    B = rbinom(1, nB, p_cancer*(1 - p_immun)),                                  # N(cancer|gene F, immunotherapy T)
    C = rbinom(1, nC, (p_gene_c + p_cancer - p_gene_c*p_cancer)),               # N(cancer|gene T, immunotherapy F)
    D = rbinom(1, nD, p_cancer)                                                 # N(cancer|gene F, immunotherapy F)
  ) %>%
  # drop intermediate calculations
  select(-one_of(c("nTreat", "nNoTreat")))

head(stim1_df)
```

Now, let's calculate causal support for each simulated data set.

```{r}
stim1_df = stim1_df %>%
  rowwise() %>%
  mutate(
    # calculate causal support for each row difference of log likelihoods is a log odds ratio
    causal_support = likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, TRUE) - likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, FALSE), 
    # convert causal support into normative probability judgment
    ground_truth = plogis(causal_support)
  )
```

While we want our stimuli to reflect natural sampling error, we also want to counterbalance that sampling error within each data condition across participants. In order to do this, we'll reduce our stimulus data from a distribution of draws to a set of quantiles of ground truth causal support that we can sample systematically.

```{r}
# generate a list of probabilities corresponding to a number of quantiles equal to the number of unique data conditions
n_quantiles <- max(stim1_df$trialIdx)
p <- ppoints(n_quantiles)

# generate a dataframe containing quantiles for each data condition
quantiles1_df <- stim1_df %>%
  group_by(trialIdx) %>%
  summarise(
    qIdx = list(seq(from = 1, to = n_quantiles, by = 1)),
    q = list(quantile(causal_support, p))
  )

# filter draws of possible stimuli to include only our quantiles
stim1_df = stim1_df %>%
  # first we need to group our stimuli dataframe the same way as our quantile dataframe without loosing any columns
  group_by(trialIdx, p_gene_c, br_gene, p_immun, br_immun, p_cancer, N) %>%
  summarise(
    draw = list(draw),
    nA = list(nA),
    nB = list(nB),
    nC = list(nC),
    nD = list(nD),
    A = list(A),
    B = list(B),
    C = list(C),
    D = list(D),
    causal_support = list(causal_support),
    ground_truth = list(ground_truth)
  ) %>%
  # join stimuli data with quantiles, and make rows represent data conditions x quantiles
  full_join(quantiles1_df, by = c("trialIdx")) %>%
  unnest(cols = c("qIdx","q")) %>%
  # find the draws that are closest to our quantiles for each data condition
  rowwise() %>%
  mutate(
    draws_diff_from_q = list(lapply(causal_support, function(x) abs(x - q) )),
    which_draw = which.min(unlist(draws_diff_from_q)),
    draw = draw[[which_draw]],
    nA = nA[[which_draw]],
    nB = nB[[which_draw]],
    nC = nC[[which_draw]],
    nD = nD[[which_draw]],
    A = A[[which_draw]],
    B = B[[which_draw]],
    C = C[[which_draw]],
    D = D[[which_draw]],
    causal_support = causal_support[[which_draw]],
    ground_truth = ground_truth[[which_draw]]
  ) %>%
  # drop the columns we no longer need at this point
  select(-one_of(c("draw", "which_draw", "q", "draws_diff_from_q")))
```

Now, let's visualize the ground truth probability that immunotherapy works, which is derived from causal support, as a function of our manipulations to see how difficult our task setup is.

We can look at the trend across trials as a line as before, but now with uncertainty due to simulated sampling error.

```{r}
stim1_df %>% 
  ggplot(aes(x = p_immun, y = ground_truth)) +
  stat_lineribbon(.width = c(0.95, 0.8, 0.5), color = "black") +
  scale_fill_brewer() +
  theme_bw() +
  facet_grid(N ~ .)
```

Alternatively, we can look at quantile dotplots of simulated trials.

```{r}
stim1_df %>% 
  ggplot(aes(x = as.factor(p_immun), y = ground_truth)) +
  stat_dots(position = "dodge", binwidth = 0.04, quantiles = n_quantiles) +
  theme_bw() +
  facet_grid(N ~ .)
```

In both charts, we can see that we are sampling a nice range of the ground truth with this trial structure. 

Let's check that half of these data conditions fall on either side of causal support of 0, ground truth of 0.5. *Finding combinations of `p_immun` and `N` that meet this criterion was a matter of trial and error.*

```{r}
sum(stim1_df$causal_support > 0) / length(stim1_df$causal_support)
```

We save this data to generate stimuli during the experiment.

```{r}
# save data as json
# write.csv(stim1_df, "stimdata1.csv", quote = FALSE)
```


## Problem 2: Does the gene that causes cancer also reduce the effectiveness of immunotherapy?

The copy for this problem will be something like: *"Scientists have found a gene that is known to increase the probability of developing a particular type of cancer. Scientists have also developed an immunotherapy which is known to reduce the probability of developing this cancer. However, in recent years, some scientists have hypothesized that the same gene known to cause cancer may also reduce the effectiveness of the immunotherapy. These data show the number of patients who do and do not develop cancer grouped based on whether or not they have the gene known to cause this cancer and whether or not they have received immunotherapy. Based on the data in the table, what is the probability that the gene reduces the is effectiveness of the immunotherapy?"*

### Monte Carlo simulation

We'll need to set up an additional Monte Carlo simulation to represent the case where there is a confounding effect of the gene on an immunotherapy that is known to be effective.

```{r}
# expects data as counts from a two-way table formatted as follows:
#   A   - count with cancer given gene T and immunotherapy T
#   nA  - total given gene T and immunotherapy T
#   B   - count with cancer given gene F and immunotherapy T
#   nB  - total given gene F and immunotherapy T
#   C   - count with cancer given gene T and immunotherapy F
#   nC  - total given gene T and immunotherapy F
#   D   - count with cancer given gene F and immunotherapy F
#   nD  - total given gene F and immunotherapy F
# the boolean immunotherapy_works determines which DAG is simulated
likelihood_from_monte_carlo_confounding <- function (A, nA, B, nB, C, nC, D, nD, immunotherapy_works = TRUE) {
  # define parameters (sample from uniform distribution)
  m <- 10000             # number of simulations
  p_cancer <- runif(m)   # base rate of cancer due to unknown causes
  p_gene_c <- runif(m)   # probability that gene causes cancer
  p_gene_i <- runif(m)   # probability that gene prevents immunotherapy from working
  if (immunotherapy_works) {
    p_immun <- runif(m)  # probability that immunotherapy prevents cancer
  } else {
    p_immun <- rep(0, m) # immunotherapy has no effect on cancer
  }
  
  
  # calculate counts for each possible outcome in the contingengy table
  counts <- cbind(
    A,
    nA - A,
    B,
    nB - B,
    C,
    nC - C,
    D,
    nD - D
  )
  
  # calculate probabilies for each possible outcome in the contingency table
  probs <- cbind(
    # p(cancer|gene T, immunotherapy T)
    (p_gene_c + p_cancer - p_gene_c*p_cancer)*((1 - p_immun) + p_immun*p_gene_i), # see notes below
    # p(~cancer|gene T, immunotherapy T)
    (1 - p_gene_c)*(1 - p_cancer) + (p_gene_c + p_cancer - p_gene_c*p_cancer)*(p_immun*(1 - p_gene_i)),
    # p(cancer|gene F, immunotherapy T)
    p_cancer*(1 - p_immun),
    # p(~cancer|gene F, immunotherapy T)
    (1 - p_cancer) + p_cancer*p_immun, 
    # p(cancer|gene T, immunotherapy F)
    p_gene_c + p_cancer - p_gene_c*p_cancer,
    # p(~cancer|gene T, immunotherapy F)
    (1 - p_gene_c)*(1 - p_cancer),
    # p(cancer|gene F, immunotherapy F)
    p_cancer, 
    # p(~cancer|gene F, immunotherapy F)
    (1 - p_cancer)
  )
  # What is the probability that immunotherapy doesn't prevent cancer in the presence of the gene? (tried the following option in sequence)
  # ((1 - p_immun)*(1 - p_gene_i) + p_immun*p_gene_i - (1 - p_immun)*p_gene_i) produces negative numbers
  # ((1 - p_immun)*(1 - p_gene_i) + p_immun*p_gene_i) removed term that is giving negative values, but then the first term seems to narrow and probably should not be conditioned on the gene. immmunotherapy that doesn't work won't interact with the gene
  # ((1 - p_immun) + p_immun*p_gene_i) removed condition on first term, so it represents probability that immunotherapy fails regardless of gene
  # ((1 - p_immun) + p_immun*p_gene_i - (1 - p_immun)*p_gene_i) tried again to account for fact that failure is never due to both causes, but this parameterization never produced positive causal support for a known data generating process. this seems obviously wrong
  # I ended up running with the third option.
  
  # calculate log likelihood of data for each of m runs of the simulation
  loglik <- rowSums(matrix(rep(counts, m), nrow = m, ncol = 8, byrow = TRUE) * log(probs))
  # normalize by maximum likelihood to make probabilities comparable across Monte Carlo simulations
  # and marginalize over simulated parameter values
  logmax <- max(loglik)
  logscore <- logmax + log(sum(exp(loglik - logmax))) - log(m)
}
```

### Trial structure

Similar to the set up for problem 1, lets generate a bunch of stimuli while only manipulating a few parameters. We'll add sampling error by drawing events from a binomial distribution, except in the case of the base rate of immunotherapy because we will always present a close-to-balanced sample. To be realistic, we'll use the actual base rate of cancer in the US in our model. *The only parameter we are adding here is that in some cases the presence of the gene prevents the immunotherapy from working. We are now holding the effectiveness of immunotherapy constant such that it prevents cancer 50% of the time.*

```{r}
# number of simulated data sets per combination of factors
n_sims <- 2000

# set up manipulations and simulate fake data
stim2_df <- expand.grid(
    p_gene_c = 0.3,                   # gene effect on cancer (control)
    p_gene_i = c(0, 0.2, 0.6, 0.8),   # gene effect on immunotherapy (4 levels)
    br_gene = 0.4,                    # base rate of gene (control)
    p_immun = 0.8,                    # immunotherapy effect (control now)
    br_immun = 0.5,                   # base rate of immunotherapy (control, no noise)
    p_cancer = 0.004424,              # actual base rate of cancer (control)
    N = c(100, 500, 2000, 5000)       # sample size (4 levels)
  ) %>%
  rowid_to_column("trialIdx") %>%
  rowwise() %>%
  mutate(
    # get a balanced sample of people do and do not receive treatment
    nTreat = round(N*br_immun),  
    nNoTreat = round(N*(1 - br_immun)),
    # sample possible numbers of people in each group who have the gene
    draw = list(seq(from = 1, to = n_sims, by = 1)),
    nA = list(rbinom(n_sims, nTreat, br_gene)),      # N(gene T, immunotherapy T)
    nC = list(rbinom(n_sims, nNoTreat, br_gene)) # N(gene T, immunotherapy F)
  ) %>%
  unnest(cols = c("draw", "nA", "nC")) %>%
  mutate(
    # calculate number of people in each group who DO NOT have the gene for each draw
    nB = nTreat - nA,  # N(gene F, immunotherapy T)
    nD = nNoTreat - nC # N(gene F, immunotherapy F)
  ) %>%
  rowwise() %>%
  mutate(
    # N(cancer|gene T, immunotherapy T)
    A = rbinom(1, nA, (p_gene_c + p_cancer - p_gene_c*p_cancer)*((1 - p_immun) + p_immun*p_gene_i)), 
    # N(cancer|gene F, immunotherapy T)
    B = rbinom(1, nB, p_cancer*(1 - p_immun)), 
    # N(cancer|gene T, immunotherapy F)
    C = rbinom(1, nC, (p_gene_c + p_cancer - p_gene_c*p_cancer)),
    # N(cancer|gene F, immunotherapy F)
    D = rbinom(1, nD, p_cancer)
  ) %>%
  # drop intermediate calculations
  select(-one_of(c("nTreat", "nNoTreat")))

head(stim2_df)
```

Now, let's calculate causal support for each simulated data set, this time assuming that the immunotherapy works and weighing evidence for or against a confounding backdoor path.

```{r}
stim2_df = stim2_df %>%
  rowwise() %>%
  mutate(
    # calculate causal support for each row (difference of log likelihoods is a log odds ratio)
    causal_support = likelihood_from_monte_carlo_confounding(A, nA, B, nB, C, nC, D, nD, TRUE) - likelihood_from_monte_carlo(A, nA, B, nB, C, nC, D, nD, TRUE), 
    # convert causal support into normative probability judgment
    ground_truth = plogis(causal_support)
  )
```

While we want our stimuli to reflect natural sampling error, we also want to counterbalance that sampling error within each data condition across participants. In order to do this, we'll reduce our stimulus data from a distribution of draws to a set of quantiles of ground truth causal support that we can sample systematically.

```{r}
# generate a list of probabilities corresponding to a number of quantiles equal to the number of unique data conditions
n_quantiles <- max(stim2_df$trialIdx)
p <- ppoints(n_quantiles)

# generate a dataframe containing quantiles for each data condition
quantiles2_df <- stim2_df %>%
  group_by(trialIdx) %>%
  summarise(
    qIdx = list(seq(from = 1, to = n_quantiles, by = 1)),
    q = list(quantile(causal_support, p))
  )

# filter draws of possible stimuli to include only our quantiles
stim2_df = stim2_df %>%
  # first we need to group our stimuli dataframe the same way as our quantile dataframe without loosing any columns
  group_by(trialIdx, p_gene_c, p_gene_i, br_gene, p_immun, br_immun, p_cancer, N) %>%
  summarise(
    draw = list(draw),
    nA = list(nA),
    nB = list(nB),
    nC = list(nC),
    nD = list(nD),
    A = list(A),
    B = list(B),
    C = list(C),
    D = list(D),
    causal_support = list(causal_support),
    ground_truth = list(ground_truth)
  ) %>%
  # join stimuli data with quantiles, and make rows represent data conditions x quantiles
  full_join(quantiles2_df, by = c("trialIdx")) %>%
  unnest(cols = c("qIdx","q")) %>%
  # find the draws that are closest to our quantiles for each data condition
  rowwise() %>%
  mutate(
    draws_diff_from_q = list(lapply(causal_support, function(x) abs(x - q) )),
    which_draw = which.min(unlist(draws_diff_from_q)),
    draw = draw[[which_draw]],
    nA = nA[[which_draw]],
    nB = nB[[which_draw]],
    nC = nC[[which_draw]],
    nD = nD[[which_draw]],
    A = A[[which_draw]],
    B = B[[which_draw]],
    C = C[[which_draw]],
    D = D[[which_draw]],
    causal_support = causal_support[[which_draw]],
    ground_truth = ground_truth[[which_draw]]
  ) %>%
  # drop the columns we no longer need at this point
  select(-one_of(c("draw", "which_draw", "q", "draws_diff_from_q")))
```

Now, let's visualize the ground truth probability that the gene prevents immunotherapy from working, which is derived from causal support, as a function of our manipulations to see how difficult our task setup is.

We can look at the trend across trials as a line as before, but now with uncertainty due to simulated sampling error.

```{r}
stim2_df %>% 
  ggplot(aes(x = p_gene_i, y = ground_truth)) +
  stat_lineribbon(.width = c(0.95, 0.8, 0.5), color = "black") +
  scale_fill_brewer() +
  theme_bw() +
  facet_grid(N ~ .)
```

Alternatively, we can look at quantile dotplots of simulated trials.

```{r}
stim2_df %>% 
  ggplot(aes(x = as.factor(p_gene_i), y = ground_truth)) +
  stat_dots(position = "dodge", binwidth = 0.04, quantiles = n_quantiles) +
  theme_bw() +
  facet_grid(N ~ .)
```

In both charts, we can see that we are sampling a nice range of the ground truth with this trial structure.

Let's check that half of these data conditions fall on either side of causal support of 0, ground truth of 0.5. *Finding combinations of `p_gene_i` and `N` that meet this criterion was a matter of trial and error.*

```{r}
sum(stim2_df$causal_support > 0) / length(stim2_df$causal_support)
```

We save this data to generate stimuli during the experiment.

```{r}
# save data as json
# write.csv(stim2_df, "stimdata2.csv", quote = FALSE)
```

## Study design

### Experiment 1

For our first experiment, we'll focus on how well participants can perform causal inference on Problem 1. I propose to show 16 trials such that we test each combination of sample size (4 levels) and immunotherapy effectiveness (4 levels). These 16 trials are our data conditions, and their order will be randomized for each participant.

At the beginning of the HIT, each participant will be assigned to a specific pairing of quantiles and data conditions, such that quantiles of sampling error are counterbalanced within each data condition. This way the fake data sets we simulated to represent natural sampling error for a given data condition will be sampled systematically across participants, rather than randomly. Hopefully, this means that (1) our sampling of causal support within participants will not be clumpy, which would lead to issues with model fit, and (2) the effects of sampling error on what users actually see will be randomized across participants, which should prevent bias in population-level inferences about how the parameters and sample sizes used to generate each data condition might impact causal reasoning beyond what is accounted for my causal support as a normative model.

This design should result in data that are realistically noisy (with contingency table data that vary enough from trial to trial) that it is believable that we are talking about a different gene, cancer, and immunotherapy on each trial. We will get balanced measurements of the impact of sample size and immunotherapy effectiveness on judgments.

Assuming causal support as a normative model, we hope to learn about the ways that causal reasoning with visualizations deviates from this norm. We will explore the following research questions by gradually adding different predictors to a model predicting human judgments from causal support:

- Does underestimating sample size, as found in prior work, result in perceived causal support which is less than the normative value (i.e., probabilities closer to 0.5)? Does the effect of sample size that is not already explained by normative causal support depend on the data presentation format?
- Do people under- or over-estimate intervention effectiveness in a way that biases their causal support judgments? Underestimation would result in probabilties that are too low and overestimation would result in probabilities that are too high. Does this pattern of bias, if it exists, depend on data presentation format?
- How does data presentation format itself influence causal induction? Is there a greater bias in some conditions? Are people's judgments more or less variable in some conditions? Specifically, how are judgments different when we show data in a *numerical table* vs a *table of icon arrays* vs a *table where the user can interactively (dis)aggregate the data by expanding or collapsing dimensions of the table* (emulating controlling for vs ignoring levels of a factor)?

### Experiment 2

Our second experiment will be a replication of the first experiment but with the much more difficult Problem 2. Instead of varying the effectiveness of immunotherapy, we will vary the probability that the gene prevents the immunotherapy from being effective.

We are looking to see if our results replicate when the causal reasoning task is to detect confounding in the presence of a known treatment effect and covariate. If results are substantially different from experiment 1, this will reveal that the biases we see do not generalize across causal reasoning scenarios. If results are similar, it suggests that people's causal reasoning is pretty stable across different tasks where they need to look for different cues in the data in order to infer the underlying structure.
